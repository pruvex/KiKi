# ğŸ“¡ Module: chat.api-connector

## ğŸ” Zweck
Dieses Modul Ã¼bernimmt die Kommunikation mit externen AI-Chat-APIs (z.â€¯B. OpenAI, Google Gemini, Claude etc.).  
Es empfÃ¤ngt Chat-Nachrichten vom UI (Ã¼ber IPC), verarbeitet diese zu API-kompatiblen Requests und sendet sie an den konfigurierten AI-Anbieter.  
AnschlieÃŸend wird die Antwort wieder zurÃ¼ck Ã¼ber IPC an das UI gegeben.

---

## ğŸ§± Architektur

- **Ort:** `main/modules/chat/api-connector.ts`
- **Schnittstelle:** IPC (`ipcMain.handle('chat:send-message', ...)`)
- **Verwendung von:**  
  - `chat.api-key-manager` â†’ zum Zugriff auf gespeicherte API-Keys  
  - `.env` oder App-Konfiguration â†’ fÃ¼r Modus (z.â€¯B. `"openai"` oder `"gemini"`)

```text
[UI]
 â”‚
 â”‚ IPC: chat:send-message
 â–¼
[Main Process]
  â””â”€â”€ chat.api-connector
        â””â”€â”€ sendet POST zu OpenAI/Gemini/etc.
             â†“
          Antwort
             â†‘
        Ãœbergabe an UI via IPC
âš™ï¸ AbhÃ¤ngigkeiten
chat.api-key-manager (fÃ¼r den API-Key-Zugriff)

node-fetch oder axios (fÃ¼r HTTP-Requests)

.env oder config-Objekt mit API-Base, Modell etc.

ğŸ”„ Ablauf (Message Flow)
UI sendet IPC-Aufruf: chat:send-message mit Nutzereingabe (message), Kontextverlauf etc.

Modul ruft den passenden API-Endpunkt auf (abhÃ¤ngig von aktivem Provider).

Antwort wird analysiert, normalisiert und an UI zurÃ¼ckgegeben.

Fehler werden sauber per IPC kommuniziert.

ğŸ“¤ IPC-Schnittstelle
chat:send-message

input: { message: string, history?: Message[], config?: {...} }

output: { reply: string, usage?: {...}, model?: string }

ğŸ§© Erweiterbarkeit
Das Modul verwendet jetzt eine Provider-Map fÃ¼r maximale FlexibilitÃ¤t. Jeder Provider (z.B. OpenAI, Gemini, lokale LLMs) erhÃ¤lt eine eigene Funktion und kann einfach Ã¼ber die Map ergÃ¤nzt werden.

Umschaltbarer Provider-Modus (provider = "openai" | "gemini" | ...), Standard ist "openai". Weitere Provider kÃ¶nnen durch Implementierung einer Funktion und einen Eintrag in die Map ergÃ¤nzt werden.

Beispiel:
```typescript
const PROVIDERS = {
  openai: callOpenAI,
  // gemini: callGemini,
  // local: callLocalLLM,
};
```
Die Hauptlogik bleibt unverÃ¤ndert, neue Provider werden modular ergÃ¤nzt.

Rate-Limit- und Fehlerhandling sind zentralisiert.

Prompt-Templating pro Provider mÃ¶glich

ğŸ§ª Teststrategie
Dummy-Modus (Mock-API): antwortet immer mit "This is a test response."

Real-Call mit Test-Key gegen echten Anbieter

Unit-Tests fÃ¼r Request-Builder & Response-Parser

ğŸ“ Dateistruktur
text
Kopieren
Bearbeiten
chat.api-connector/
â”‚
â”œâ”€ README.md            â† diese Datei
â”œâ”€ prompt.md            â† Prompt fÃ¼r AI Studio
â”œâ”€ response.md          â† KI-generierter Code
â””â”€ status.md            â† Fortschrittsdokumentation
ğŸ—‚ Modulstatus
 README erstellt

 Prompt erstellt

 AI-Code generiert (response.md)

 Implementierung geprÃ¼ft

 Getestet & committed

yaml
Kopieren
Bearbeiten
